â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                            ğŸš€ HessGPT_OP v4 - TECHNICAL OVERVIEW
                      (Mistral Tokenizer + Dual Model + Production Ready)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“¦ DUAL MODEL ARCHITECTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

System:                     Hybrid Dual-Model (Specialized Routing)

MODEL 1: HessGPT-Factual (0.5B params)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Role:                       Primary assistant (factual, conversational, general)
Specialization:             - Factual knowledge (Wikipedia, encyclopedic)
                           - Conversational assistance (polite, structured)
                           - Task decomposition (how-to, step-by-step)
                           - General reasoning
Training Data:              - DCLM (25%): Conversational diversity
                           - RefinedWeb (15%): Premium web
                           - FineWeb-Edu PDF-like (20%): Academic structure
                           - FineWeb-Edu (10%): Educational accessible
                           - Cosmopedia v2 (20%): Synthetic high-quality
                           - Wikipedia (10%): Factual knowledge
                           Total: ~10B tokens (0% code, 0% math)
Routing Capability:         YES - Generates routing tokens when code/math needed

MODEL 2: HessGPT-Code (0.3B params)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Role:                       Specialized code/math/logic engine
Specialization:             - Python (50-60%): Algorithms, calculations, data
                           - JavaScript (20-25%): Logic, web
                           - HTML (10-12%): Structure
                           - CSS (8-10%): Styling
Training Data:              Code-specific datasets (Python, JS, HTML, CSS)
Execution Environment:      Python sandbox (secure execution)
Routing:                    Receives requests from Model 1 via routing tokens

ROUTING MECHANISM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model 1 (0.5B) generates routing token â†’ Router detects â†’ Model 2 (0.3B) executes
â†’ Result returned to Model 1 â†’ Final response integrated

Example:
User: "Calculate the derivative of xÂ²+3x"
Model 1: <code>Calculate the derivative of xÂ²+3x<|end|>
  â†’ Router detects <|math|> â†’ Send to Model 2
Model 2: <code>2x+3<|end|>
  â†’ Return to Model 1
Model 1: "The derivative is <code>2x+3<|end|>"


ğŸ“¦ MODEL 1 CONFIGURATION (HessGPT-Factual 0.5B)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Architecture:           Decoder-Only Transformer (GPT-style)
Model Size:             ~500M parameters (0.5B)
Embedding Dimension:    1280
Number of Layers:       22 (optimized for faster training)
Query Heads:            20 (for attention)
KV Heads:               5 (GQA - Grouped Query Attention)
GQA Ratio:              4:1 (20 Q heads â†’ 5 KV heads)
Vocabulary Size:        32,008 (Mistral 32k + 8 special tokens)
Tokenizer:              mistralai/Mistral-7B-v0.1 (SentencePiece)
Context Length:         
  - Pretrain:           1,024 tokens
  - SFT (with YaRN):    4,096 tokens (4x extension)


ğŸ”§ CORE TECHNOLOGIES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. âœ… RMSNorm (Root Mean Square Normalization)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  Simplified normalization without mean centering
   â€¢ Formula:      RMSNorm(x) = x / RMS(x) * Î³
   â€¢ Benefits:     - Faster computation than LayerNorm
                   - Simpler (no beta parameter)
                   - Same training quality as LayerNorm
                   - Used in LLaMA, Mistral, Qwen, GPT-NeoX
   â€¢ Application:  Pre-attention and pre-FFN in each layer
   â€¢ Parameters:   Only weight (gamma), no bias
   â€¢ Status:       ACTIVE (pretrain & SFT)


2. âœ… Flash Attention (PyTorch 2.0+ Optimized)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  Memory-efficient attention with automatic fallback
   â€¢ Implementation: F.scaled_dot_product_attention (PyTorch 2.0+)
   â€¢ Benefits:     - 2-3x speedup on H100
                   - O(N) memory instead of O(NÂ²)
                   - Automatic kernel selection
                   - Handles scaling, masking, dropout automatically
   â€¢ Fallback:     Standard attention for PyTorch < 2.0
   â€¢ Status:       ACTIVE (H100), FALLBACK (RTX 5050/older GPUs)


3. âœ… RoPE (Rotary Position Embeddings)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  Positional encoding via rotation matrices
   â€¢ Benefits:     - No learned position embeddings (saves params)
                   - Better extrapolation to longer sequences
                   - Relative position awareness
   â€¢ Implementation: Applied in attention before QK product
   â€¢ Base:         10,000 (frequency base)
   â€¢ Status:       ACTIVE (pretrain & SFT)


4. âœ… YaRN (Yet another RoPE extensioN)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  Context length extension for RoPE
   â€¢ Extension:    1,024 â†’ 4,096 tokens (4x scale)
   â€¢ Method:       Non-uniform frequency scaling
                   - High frequencies: minimal scaling (local info)
                   - Low frequencies: strong scaling (global info)
   â€¢ Transition:   Beta = 32 (transition point)
   â€¢ Attention:    Attention scores scaled by sqrt(yarn_scale)
   â€¢ Benefits:     - Smooth transition from pretrain to SFT
                   - Preserves pretrained knowledge
                   - No architectural changes needed
   â€¢ Status:       DISABLED (pretrain) â†’ ENABLED (SFT)
   â€¢ Scale factor: 4.0 (1024 â†’ 4096)


5. âœ… GQA (Grouped Query Attention)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  Reduced KV heads for efficiency
   â€¢ Configuration: 20 Query heads â†’ 5 KV heads (4:1 ratio)
   â€¢ Mechanism:    Each KV head is shared across 4 query heads
   â€¢ Benefits:     - 37.5% reduction in attention parameters
                   - Faster inference (smaller KV cache)
                   - Minimal quality loss vs MHA
                   - Better than Multi-Query Attention (MQA)
   â€¢ Savings:      ~885K parameters per attention layer
   â€¢ Status:       ACTIVE (pretrain & SFT)


6. âœ… SwiGLU Activation
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  Gated Linear Unit with Swish (SiLU) activation
   â€¢ Formula:      SwiGLU(x) = Swish(xW_gate) âŠ™ (xW_up)
                   where Swish(x) = x * sigmoid(x)
   â€¢ Architecture: - Gate projection (W1)
                   - Up projection (W2)
                   - Down projection (W3)
   â€¢ FFN expansion: 8/3x (1280 â†’ 3413 â†’ 1280)
                   Instead of 4x for GELU to keep similar param count
   â€¢ Benefits:     - Better performance than ReLU/GELU
                   - Smoother gradients
                   - Used in LLaMA, PaLM, Mistral
   â€¢ Status:       ACTIVE (pretrain & SFT)


7. âœ… QK-Norm (Query-Key Normalization)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  RMSNorm on Query and Key before attention computation
   â€¢ Benefits:     - Stabilizes attention scores
                   - Enables higher learning rates (4e-4)
                   - Better training dynamics
                   - Prevents attention score explosion
   â€¢ Implementation: Separate RMSNorm for Q and K (head_dim normalization)
   â€¢ Overhead:     +256 parameters per attention layer (minimal)
   â€¢ Status:       ENABLED (pretrain & SFT) - Critical for 4e-4 LR


8. âœ… Soft-Capping (Logits)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  Smooth limiting of logit magnitudes
   â€¢ Formula:      soft_cap(x) = cap * tanh(x / cap)
   â€¢ Cap value:    30.0
   â€¢ Benefits:     - Prevents extreme logit values
                   - Training stability (Gemma-style)
                   - Better gradient flow
                   - Smoother probability distributions
   â€¢ Inspiration:  Google Gemma
   â€¢ Status:       ACTIVE (pretrain & SFT)


9. âœ… Weight Tying
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Description:  Shared weights between token embeddings and output head
   â€¢ Implementation: output_head.weight = token_embeddings.weight
   â€¢ Benefits:     - Saves ~64.3M parameters (vocab_size Ã— embed_dim)
                   - Regularization effect
                   - Standard in modern LLMs
   â€¢ Status:       ACTIVE (pretrain & SFT)


10. âœ… Causal Mask Caching
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Description:  Pre-computed and cached triangular attention mask
    â€¢ Implementation: Cached as buffer, auto-expands for longer sequences
    â€¢ Mask format:  Boolean tensor (True = masked, False = visible)
    â€¢ Benefits:     - No mask recomputation per forward pass
                    - Auto-expansion for variable sequence lengths
                    - Memory efficient
    â€¢ Status:       ACTIVE (automatic)


11. âœ… Vocabulary & Special Tokens
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Tokenizer:    Mistral-7B-v0.1 (SentencePiece, 32k vocab)
    â€¢ Base vocab:   32,000 (Mistral base)
    â€¢ Special tokens: 8 additional tokens
    
    CHAT TOKENS:
      - <|system|>    â†’ 32000 (system prompts)
      - <|user|>      â†’ 32001 (user messages)
      - <|assistant|> â†’ 32002 (assistant responses)
      - <|end|>       â†’ 32003 (end of turn)
    
    ROUTING TOKENS (Dual Model Architecture):
      - <|code|>      â†’ 32004 (Python/JS/HTML/CSS code request)
      - <|math|>      â†’ 32005 (Equations, calculations)
      - <|logic|>     â†’ 32006 (Complex logical reasoning)
      - <|result|>    â†’ 32007 (Result from specialized model)
    
    â€¢ Total vocab:  32,008
    â€¢ Benefits:     - 36% more efficient than GPT-2 (32k vs 50k)
                    - Better multilingual support
                    - SentencePiece > BPE (GPT-2)
                    - Routing tokens enable Model 1 â†” Model 2 communication
    â€¢ Status:       ACTIVE


ğŸ“Š PARAMETER BREAKDOWN
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Component                   Parameters          Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Token Embeddings            40,970,240          vocab_size (32,008) Ã— embed_dim (1280)
Position Embeddings         0                   RoPE (no learned params)
Transformer Blocks (22)     ~435M               With GQA, SwiGLU, RMSNorm
  Per Block:                ~19.8M
    - Attention (GQA)       ~9.8M               37.5% less than MHA
    - FFN (SwiGLU)          ~10.0M              8/3x expansion
    - RMSNorm (2x)          2,560               2 Ã— embed_dim (pre-attn, pre-ffn)
    - QK-Norm (optional)    256                 2 Ã— head_dim (if enabled)
Final Norm (RMSNorm)        1,280               embed_dim
Output Head                 0                   Weight tying (shared with embeddings)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL (Model 1)             ~476,000,000        (0.476B parameters - Factual/Assistant)

GQA Savings per layer:      ~885,000            vs standard MHA
Total GQA savings:          ~19.5M              across 22 layers
Tokenizer efficiency:       32k vocab (36% smaller than GPT-2's 50k)


ğŸ¯ TRAINING CONFIGURATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PHASE 1: PRETRAIN (MODEL 1 - Factual/Assistant 0.5B)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Context Length:             1,024 tokens
YaRN:                       Disabled
Dropout:                    0.0 (standard for large-scale pretraining)
Optimizer:                  AdamW (fused on CUDA)
  - Beta1:                  0.9
  - Beta2:                  0.95
  - Epsilon:                1e-8
Learning Rate:              4e-4 (aggressive but stable with QK-Norm)
Weight Decay:               0.1 (standard for deep models)
LR Schedule:                WSD (Warmup-Stable-Decay)
  - Warmup ratio:           3% (better for interrupted training)
  - Decay ratio:            15% (smoother end-of-chunk transitions)
  - Min LR ratio:           10% (4e-5 minimum)
Batch Size:                 32
Gradient Accumulation:      4
Effective Batch:            128 samples
Gradient Clipping:          1.0 (prevents gradient explosion)
Mixed Precision:            BF16 autocast (FP32 weights)

DATA CONFIGURATION:
  - Total chunks:           10 (1B tokens each)
  - Train/Val split:        Per-chunk automatic split
    â€¢ Train:                985M tokens (98.5%)
    â€¢ Val:                  15M tokens (1.5%)
  - Split method:           Random shuffle per chunk
  - Total train data:       9.85B tokens
  - Total val data:         150M tokens (10 chunks Ã— 15M)
  - Validation frequency:   Every 500 steps
  - Val batches per check:  50 batches
  
  Benefits:
    â€¢ Each chunk self-contained (no separate val chunk)
    â€¢ Val samples rotate per epoch (10 different val sets)
    â€¢ 50 GB RAM instead of 100 GB (50% savings)
    â€¢ +850M tokens for training vs dedicated val chunk

TOKENIZATION:
  - Format:                 .npy (NumPy arrays)
  - Loading speed:          2-5 sec per chunk (vs 11-22 min with .pt)
  - RAM efficiency:         Direct memory mapping
  - Tokenizer:              Mistral-7B-v0.1 (32k vocab)

VALIDATION STRATEGY:
  Each chunk provides its own validation set:
    Epoch 1: Train on chunk 000 (985M), validate on chunk 000 (15M)
    Epoch 2: Train on chunk 001 (985M), validate on chunk 001 (15M)
    ...
  â†’ Validation loss averaged across all 10 chunks
  â†’ Detects chunk-specific overfitting
  â†’ More robust than single validation set
  - Min LR ratio:           10% (prevents collapse on resume)
Gradient Clipping:          1.0 (max_grad_norm)
Batch Size:                 32 (per GPU)
Gradient Accumulation:      4 steps (effective batch = 128)
Mixed Precision:            BF16 (autocast)
Compilation:                torch.compile (mode='default')
Flash Attention:            Enabled (H100 only)


PHASE 2: SFT (Supervised Fine-Tuning)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Context Length:             4,096 tokens (YaRN extension)
YaRN:                       Enabled (scale=4.0)
Method:                     LoRA (recommended for efficiency)
LoRA Rank:                  16-64 (H100: can go higher)
LoRA Alpha:                 32-128 (2x rank typically)
Target Modules:             q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj
Trainable Params:           ~1-5M (LoRA) vs 500M (full fine-tune)
Learning Rate:              Lower than pretrain (e.g., 1e-4)


ğŸ”„ COMPATIBILITY & WORKFLOW
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… Pretrain â†’ SFT Transition:
   1. Load pretrained checkpoint (1024 context)
   2. Enable YaRN in model config (yarn_scale=4.0, use_yarn=True)
   3. Model automatically handles extended context (4096 tokens)
   4. No weight reinitialization needed
   5. Attention scores auto-scaled by sqrt(yarn_scale)
   6. Weights fully compatible

âœ… Checkpoint Format:
   â€¢ PyTorch state_dict (.pt)
   â€¢ Includes:
     - model_state_dict
     - optimizer_state_dict
     - scheduler_state_dict
     - global_step
     - training_history
     - config metadata
   â€¢ Safetensors (recommended for deployment)
   â€¢ HuggingFace compatible (with conversion script)

âœ… Resume Training:
   â€¢ Automatic checkpoint detection
   â€¢ LR synchronization after resume
   â€¢ Chunk-based training friendly
   â€¢ Training history preserved


ğŸ’¾ HARDWARE REQUIREMENTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TRAINING (H100 80GB):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPU:                        H100 80GB VRAM
VRAM Usage:                 
  - Pretrain:               ~35-45 GB (batch=32, grad_accum=4)
  - SFT LoRA:               ~20-25 GB (batch=16)
  - Flash Attention saves:  ~30% VRAM vs standard attention
RAM:                        64-128 GB (recommended for data loading)
Storage:                    
  - Per checkpoint:         ~2 GB
  - Training data:          Variable (chunk-based)
Speedup:                    
  - Flash Attention:        2-3x faster on H100
  - torch.compile:          10-20% additional speedup


TRAINING FALLBACK (RTX 5050/Consumer GPU):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPU:                        16GB+ VRAM
Flash Attention:            Disabled (automatic fallback)
Batch Size:                 Reduced (8-16)
Gradient Accumulation:      Increased (8-16)
Mixed Precision:            FP16/BF16 (if supported)


INFERENCE (Consumer GPU):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPU:                        RTX 3090/4090 (24GB) or better
VRAM Usage:                 
  - FP32:                   ~8 GB
  - FP16:                   ~4 GB
  - INT8 quantized:         ~2 GB
  - INT4 quantized:         ~1.5 GB
CPU Inference:              Possible but slower (llama.cpp compatible)


âš¡ PERFORMANCE OPTIMIZATIONS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… RMSNorm vs LayerNorm:
   â€¢ 15-20% faster normalization
   â€¢ Same training quality
   â€¢ Fewer parameters (no beta)

âœ… Flash Attention (H100):
   â€¢ 2-3x faster attention computation
   â€¢ 30% VRAM savings
   â€¢ O(N) memory complexity

âœ… GQA (Grouped Query Attention):
   â€¢ 37.5% fewer attention parameters
   â€¢ 4x smaller KV cache during inference
   â€¢ Faster generation (especially long context)

âœ… torch.compile:
   â€¢ 10-20% additional speedup
   â€¢ Kernel fusion
   â€¢ Graph optimization

âœ… Weight Tying:
   â€¢ 64.3M parameters saved
   â€¢ Faster forward pass
   â€¢ Better memory locality

âœ… Causal Mask Caching:
   â€¢ No recomputation overhead
   â€¢ Memory efficient buffer
   â€¢ Automatic expansion


ğŸ”¬ VALIDATION & TESTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… Code Quality:
   â€¢ All components have unit tests
   â€¢ Production-ready error handling
   â€¢ Automatic fallbacks (Flash Attention)
   â€¢ Parameter validation on init

âœ… Verified Features:
   â€¢ RMSNorm implementation âœ“
   â€¢ Flash Attention with fallback âœ“
   â€¢ GQA parameter reduction âœ“
   â€¢ QK-Norm stabilization âœ“
   â€¢ Soft-capping bounds âœ“
   â€¢ Weight tying âœ“
   â€¢ Causal mask caching âœ“
   â€¢ YaRN context extension âœ“
   â€¢ Checkpoint resume âœ“
   â€¢ Vocab resize support âœ“

âœ… Bug Fixes (v3):
   â€¢ Loss uses ignore_index for padding âœ“
   â€¢ Causal mask uses boolean directly âœ“
   â€¢ Mask cached (not recreated) âœ“
   â€¢ YaRN attention scaling applied âœ“
   â€¢ LR synchronized after resume âœ“
   â€¢ Chunk ID parsing robust âœ“


ğŸ†• VERSION 4 CHANGES (February 2025)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

MAJOR UPGRADES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Mistral Tokenizer Migration
   â€¢ Switched from GPT-2 (50k) to Mistral-7B-v0.1 (32k)
   â€¢ SentencePiece > BPE (more efficient)
   â€¢ 36% smaller vocab â†’ 20% faster tokenization
   â€¢ Better multilingual coverage (even for English-only model)
   â€¢ Vocab: 32,008 (32k base + 8 special tokens)

2. Dual Model Architecture (Specialized Routing)
   â€¢ Model 1 (0.5B): Factual/Assistant/General
   â€¢ Model 2 (0.3B): Code/Math/Logic specialist
   â€¢ Routing tokens: <|code|>, <|math|>, <|logic|>, <|result|>
   â€¢ Automatic task delegation from Model 1 to Model 2
   â€¢ Python sandbox execution for Model 2
   â€¢ Languages: Python (50-60%), JS (20-25%), HTML (10-12%), CSS (8-10%)

3. Per-Chunk Train/Val Split
   â€¢ Each chunk: 985M train + 15M val (98.5% / 1.5%)
   â€¢ Random shuffle split per chunk
   â€¢ No dedicated validation chunk needed
   â€¢ 50% RAM savings (50 GB instead of 100 GB)
   â€¢ +850M training tokens (9.85B vs 9B)
   â€¢ Validation rotates across 10 chunks â†’ more robust

4. Ultra-Fast Data Loading (.npy format)
   â€¢ NumPy arrays instead of PyTorch .pt pickles
   â€¢ Loading speed: 2-5 sec (vs 11-22 min with .pt) - 600x faster!
   â€¢ Direct memory mapping support
   â€¢ 3 hours of GPU time saved per 10 chunks
   â€¢ Critical for 12h/week GPU budget (H100)

5. Bug Fixes (Critical)
   â€¢ Soft-capping APPLIED in forward pass (was stored but not used)
   â€¢ SFT masking uses SPECIAL_TOKENS dict (not tokenizer.encode)
   â€¢ Dynamic padding in SFT (70-80% memory savings)
   â€¢ Multi-turn masking for conversations
   â€¢ YaRN scale validation warnings

ARCHITECTURE IMPROVEMENTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ All v3 features preserved (RMSNorm, Flash, GQA, QK-Norm, etc.)
â€¢ 8 routing tokens for Model 1 â†” Model 2 communication
â€¢ Optimized for factual/conversational specialization
â€¢ Code/math offloaded to specialized 0.3B model
â€¢ Training dataset: 0% code, 0% math (pure factual/assistant)

DATA PIPELINE v4:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Mistral tokenizer for consistency
â€¢ Deduplication enabled (hash-based)
â€¢ Academic filter for FinePDFs dataset
â€¢ Distribution: 25-15-20-10-20-10 (optimized for factual)
  - DCLM: 25% (conversational diversity)
  - RefinedWeb: 15% (premium web)
  - FineWeb-Edu PDF-like: 20% (academic structure)
  - FineWeb-Edu: 10% (educational accessible)
  - Cosmopedia v2: 20% (synthetic high-quality)
  - Wikipedia: 10% (factual knowledge)

TRAINING EFFICIENCY v4:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Chunk loading: 2-5 sec (was 11-22 min) â†’ ~18 min saved per chunk
â€¢ Total GPU savings: ~3 hours on 10 chunks (25% of 12h budget)
â€¢ RAM usage: 50 GB (was 100 GB) â†’ allows bigger batch sizes
â€¢ Validation: Rotates across chunks â†’ better generalization detection


ğŸ“š REFERENCES & INSPIRATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â€¢ RoPE:         "RoFormer: Enhanced Transformer with Rotary Position Embedding"
â€¢ YaRN:         "YaRN: Efficient Context Window Extension of LLMs"
â€¢ GQA:          "GQA: Training Generalized Multi-Query Transformer Models"
â€¢ SwiGLU:       "GLU Variants Improve Transformer" + Swish activation
â€¢ Soft-capping: Google Gemma technical report
â€¢ RMSNorm:      "Root Mean Square Layer Normalization"
â€¢ Flash Attn:   "FlashAttention: Fast and Memory-Efficient Exact Attention"
â€¢ QK-Norm:      Various papers on attention stabilization


ğŸ—ï¸ ARCHITECTURE SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

HessGPT_OP v4 is a production-ready dual-model system combining:

âœ… Modern Efficiency:
   â€¢ Mistral tokenizer (32k vocab, 36% smaller, 20% faster)
   â€¢ RMSNorm (15-20% faster than LayerNorm)
   â€¢ Flash Attention (2-3x speedup on H100)
   â€¢ GQA (37.5% fewer params, 4x smaller KV cache)
   â€¢ Weight tying (40.9M params saved with Mistral vocab)
   â€¢ Ultra-fast .npy loading (600x faster than .pt)

âœ… Specialized Architecture:
   â€¢ Model 1 (0.476B): Factual/Assistant/Conversational
     - Training: 0% code, 0% math (pure factual)
     - Datasets: DCLM, RefinedWeb, FineWeb-Edu, Cosmopedia, Wikipedia
   â€¢ Model 2 (0.3B): Code/Math/Logic specialist
     - Languages: Python, JavaScript, HTML, CSS
     - Sandbox: Secure Python execution
   â€¢ Routing: Automatic task delegation via special tokens

âœ… Training Stability:
   â€¢ QK-Norm (enables 4e-4 LR)
   â€¢ Soft-capping (prevents logit explosion) - FIXED in v4
   â€¢ WSD scheduler (chunk-friendly)
   â€¢ Robust checkpointing
   â€¢ Per-chunk train/val split (15M val per chunk)

âœ… Resource Optimization:
   â€¢ RAM: 50 GB (vs 100 GB in v3) - 50% savings
   â€¢ Loading: 2-5 sec per chunk (vs 11-22 min) - 600x faster
   â€¢ Training data: 9.85B tokens (vs 9B) - +9.4% more data
   â€¢ GPU time saved: ~3 hours per 10 chunks (critical for 12h/week budget)

âœ… Context Flexibility:
   â€¢ RoPE (no learned positions)
   â€¢ YaRN (seamless 1024â†’4096 extension)
   â€¢ Pretrain-to-SFT compatible

âœ… Production Quality:
   â€¢ Comprehensive error handling
   â€¢ Automatic fallbacks
   â€¢ Parameter validation
   â€¢ Resume-friendly training
   â€¢ Bug-free (v4 fixes all critical issues)

Target: Dual-model system optimized for:
â€¢ Model 1 (0.5B): Fast, factual, conversational assistant
â€¢ Model 2 (0.3B): Specialized code/math/logic execution
â€¢ Fast training on H100 (Flash Attention + .npy loading)
â€¢ Efficient inference (GQA, weight tying)
â€¢ Flexible context (RoPE + YaRN)
â€¢ Stable training at high LR (QK-Norm, soft-capping)
â€¢ Task routing (automatic delegation to specialist model)


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                                Generated: February 2025
                           Architecture: HessGPT_OP v4.0
            (Mistral Tokenizer + Dual Model + Ultra-Fast Loading + Production Ready)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”