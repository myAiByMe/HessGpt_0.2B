â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                    ğŸš€ HessGPT DUAL MODEL - TRAINING ROADMAP
                  (Model 0.5B Factual + Model 0.3B Code + Router)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


ğŸ“‹ VUE D'ENSEMBLE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ARCHITECTURE FINALE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  USER INPUT                                                                â”‚
â”‚      â†“                                                                     â”‚
â”‚  Model 1 (0.5B - Factual/Assistant)                                       â”‚
â”‚      â†“                                                                     â”‚
â”‚  DÃ©tecte besoin de code/math ?                                            â”‚
â”‚      â”œâ”€ NON â†’ RÃ©ponse directe                                             â”‚
â”‚      â””â”€ OUI â†’ GÃ©nÃ¨re <|code|>prompt<|end|>                                â”‚
â”‚                â†“                                                           â”‚
â”‚            ROUTER (script Python)                                          â”‚
â”‚                â†“                                                           â”‚
â”‚            Model 2 (0.3B - Code Specialist)                                â”‚
â”‚                â†“                                                           â”‚
â”‚            Python Sandbox (exÃ©cution sÃ©curisÃ©e)                            â”‚
â”‚                â†“                                                           â”‚
â”‚            RÃ©sultat â†’ Router â†’ Model 1                                     â”‚
â”‚                â†“                                                           â”‚
â”‚            RÃ©ponse finale intÃ©grÃ©e                                         â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


TOKENS SPÃ‰CIAUX:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
<|code|>    (32004)  - Demande d'exÃ©cution de code/math/logic
<|end|>     (32003)  - Fin du bloc
<|result|>  (32007)  - RÃ©sultat du Model 2 (optionnel)


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 1: TRAINING MODEL 1 (0.5B - FACTUAL/ASSISTANT)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ã‰TAPE 1.1: PRE-TRAIN (10B tokens, 0% code/math)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dataset:
  - DCLM (25%)
  - RefinedWeb (15%)
  - FineWeb-Edu PDF (20%)
  - FineWeb-Edu (10%)
  - Cosmopedia v2 (20%)
  - Wikipedia (10%)

Commande:
  python download_ultra_filtered_mistral.py
  python pretrain_hessgpt_mistral.py

Output:
  ./checkpoints/HessGpt_pretrain.pt (Model 1 pre-trained)

DurÃ©e:
  ~5 jours (12h GPU Ã— 10 chunks)


Ã‰TAPE 1.2: SFT LoRA #1 (Conversational)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dataset:
  - SmolTalk (70k)
  - WikiHow (20k)
  - Tulu 3 Personas (30k)
  - LongAlign (10k)
  Total: 130k samples

Commande:
  python sft_hessgpt_mistral_lora.py \
    --pretrain-checkpoint ./checkpoints/HessGpt_pretrain.pt \
    --output-checkpoint ./checkpoints/HessGpt_sft1_lora.pt

Output:
  ./checkpoints/HessGpt_sft1_lora.pt (LoRA weights)

DurÃ©e:
  ~12h (1 session GPU)


Ã‰TAPE 1.3: MERGE LoRA #1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Objectif: Fusionner LoRA dans le modÃ¨le de base

Script Ã  crÃ©er: merge_lora.py

Commande:
  python merge_lora.py \
    --base-model ./checkpoints/HessGpt_pretrain.pt \
    --lora-weights ./checkpoints/HessGpt_sft1_lora.pt \
    --output ./checkpoints/HessGpt_sft1_merged.pt

Output:
  ./checkpoints/HessGpt_sft1_merged.pt (Model 1 conversational)

DurÃ©e:
  ~5-10 min (CPU)


Ã‰TAPE 1.4: SFT LoRA #2 (Routing Training) âš¡ CRITIQUE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Objectif: Apprendre au Model 1 Ã  gÃ©nÃ©rer <|code|> quand nÃ©cessaire

Dataset spÃ©cial (20k samples):
  Format:
    {
      "messages": [
        {"role": "user", "content": "Calcule 2+2"},
        {"role": "assistant", "content": "<|code|>\nprint(2+2)\n<|end|>\nLe rÃ©sultat est 4."}
      ]
    }

Exemples:
  - User: "Quelle est la dÃ©rivÃ©e de xÂ²?"
    Assistant: "<|code|>\nimport sympy as sp\nx = sp.Symbol('x')\nsp.diff(x**2, x)\n<|end|>\nLa dÃ©rivÃ©e est 2x."
  
  - User: "Ã‰cris une fonction Fibonacci"
    Assistant: "<|code|>\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n<|end|>\nVoici la fonction."

GÃ©nÃ©ration dataset:
  â†’ Utilise une IA (GPT-4, Claude) pour gÃ©nÃ©rer 20k exemples
  â†’ Format: questions qui nÃ©cessitent code â†’ rÃ©ponses avec <|code|>

Commande:
  python sft_hessgpt_mistral_lora.py \
    --pretrain-checkpoint ./checkpoints/HessGpt_sft1_merged.pt \
    --output-checkpoint ./checkpoints/HessGpt_routing_lora.pt \
    --dataset ./data/routing_dataset_20k.json

Output:
  ./checkpoints/HessGpt_routing_lora.pt (LoRA routing)

DurÃ©e:
  ~2-3h


Ã‰TAPE 1.5: MERGE LoRA #2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Commande:
  python merge_lora.py \
    --base-model ./checkpoints/HessGpt_sft1_merged.pt \
    --lora-weights ./checkpoints/HessGpt_routing_lora.pt \
    --output ./checkpoints/HessGpt_model1_final.pt

Output:
  ./checkpoints/HessGpt_model1_final.pt âœ… MODEL 1 FINAL

DurÃ©e:
  ~5-10 min


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 2: TRAINING MODEL 2 (0.3B - CODE SPECIALIST)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ã‰TAPE 2.1: CRÃ‰ER CONFIG MODEL 2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Config HessGPT 0.3B:
  {
    'vocab_size': 32008,
    'embed_dim': 896,      # RÃ©duit (vs 1280)
    'num_heads': 14,       # RÃ©duit (vs 20)
    'num_layers': 16,      # RÃ©duit (vs 22)
    'max_seq_len': 2048,   # Plus court (code < conversational)
    'n_kv_heads': 2,       # Ratio 7:1 (vs 4:1)
    ...
  }

Total params: ~300M


Ã‰TAPE 2.2: PRE-TRAIN MODEL 2 (Code datasets)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dataset code (5-7B tokens):
  - The Stack (Python filtered): 50-60%
  - The Stack (JavaScript): 20-25%
  - The Stack (HTML): 10-12%
  - The Stack (CSS): 8-10%

Filtres:
  âœ… Code fonctionnel (pas de snippets cassÃ©s)
  âœ… CommentÃ©
  âœ… Pas de malware
  âœ… Tests inclus (si possible)

Script Ã  crÃ©er: download_code_dataset.py

Commande:
  python download_code_dataset.py --output ./data/code_filtered
  python pretrain_hessgpt_mistral.py \
    --config ./configs/model2_config.json \
    --data-dir ./data/code_filtered \
    --output ./checkpoints/HessGpt_model2_pretrain.pt

DurÃ©e:
  ~3-4 jours (moins de chunks, model plus petit)


Ã‰TAPE 2.3: SFT MODEL 2 (Code instruction)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dataset:
  - Code Alpaca (Python, JS, HTML, CSS)
  - CodeContests
  - APPS (code problems)
  Total: ~50k samples

Format:
  {
    "instruction": "Write a function to reverse a string",
    "output": "def reverse_string(s):\n    return s[::-1]"
  }

Commande:
  python sft_hessgpt_mistral_lora.py \
    --pretrain-checkpoint ./checkpoints/HessGpt_model2_pretrain.pt \
    --output-checkpoint ./checkpoints/HessGpt_model2_sft_lora.pt \
    --dataset ./data/code_instruction_50k.json

DurÃ©e:
  ~6-8h


Ã‰TAPE 2.4: MERGE MODEL 2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Commande:
  python merge_lora.py \
    --base-model ./checkpoints/HessGpt_model2_pretrain.pt \
    --lora-weights ./checkpoints/HessGpt_model2_sft_lora.pt \
    --output ./checkpoints/HessGpt_model2_final.pt

Output:
  ./checkpoints/HessGpt_model2_final.pt âœ… MODEL 2 FINAL


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 3: ROUTER + PYTHON SANDBOX
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ã‰TAPE 3.1: CRÃ‰ER PYTHON SANDBOX
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Objectif: ExÃ©cuter code Python de faÃ§on sÃ©curisÃ©e

Options:
  1. RestrictedPython (simple, limitÃ©)
  2. Docker container (sÃ©curisÃ©, overhead)
  3. PyPy sandbox (Ã©quilibre)

Script Ã  crÃ©er: sandbox.py

FonctionnalitÃ©s:
  - Timeout (5 sec max)
  - Limite mÃ©moire (100 MB)
  - Pas d'accÃ¨s filesystem
  - Pas d'imports dangereux (os, subprocess, etc.)
  - Whitelist: math, numpy, sympy (calculs)

Exemple:
  result = sandbox.execute("""
  import math
  print(math.sqrt(16))
  """)
  # Output: "4.0"


Ã‰TAPE 3.2: CRÃ‰ER ROUTER
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Script: router.py

Workflow:
  1. User input â†’ Model 1
  2. Model 1 gÃ©nÃ¨re rÃ©ponse
  3. Router dÃ©tecte <|code|> ?
     - NON â†’ Retourne rÃ©ponse Model 1
     - OUI â†’ Extract code â†’ Model 2 â†’ Sandbox â†’ Inject rÃ©sultat â†’ Retourne
  
Pseudo-code:
  def route(user_input):
      # Generate avec Model 1
      response1 = model1.generate(user_input)
      
      # Check for <|code|> token
      if '<|code|>' in response1:
          # Extract code block
          code = extract_between('<|code|>', '<|end|>', response1)
          
          # Send to Model 2 for refinement (optionnel)
          code_refined = model2.generate(code)
          
          # Execute in sandbox
          result = sandbox.execute(code_refined)
          
          # Inject result
          final = response1.replace(
              f'<|code|>{code}<|end|>',
              f'<|code|>{code}<|end|>\nRÃ©sultat: {result}'
          )
          return final
      else:
          return response1


Ã‰TAPE 3.3: INFERENCE SCRIPT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Script: inference.py

Usage:
  python inference.py \
    --model1 ./checkpoints/HessGpt_model1_final.pt \
    --model2 ./checkpoints/HessGpt_model2_final.pt \
    --prompt "Calcule la racine carrÃ©e de 144"

Output:
  Model 1: "<|code|>\nimport math\nprint(math.sqrt(144))\n<|end|>"
  Router: DÃ©tecte <|code|>
  Sandbox: ExÃ©cute â†’ "12.0"
  Final: "La racine carrÃ©e de 144 est 12.0"


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 4: DATASET ROUTING (20K SAMPLES)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FORMAT DATASET:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{
  "messages": [
    {
      "role": "user",
      "content": "Calcule 15 * 23"
    },
    {
      "role": "assistant",
      "content": "<|code|>\nresult = 15 * 23\nprint(result)\n<|end|>\nLe rÃ©sultat est 345."
    }
  ]
}


CATÃ‰GORIES (20k samples):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Calculs mathÃ©matiques (5k)
   - ArithmÃ©tique simple
   - Ã‰quations
   - DÃ©rivÃ©es, intÃ©grales (avec sympy)
   - Statistiques

2. Algorithmes Python (5k)
   - Tri, recherche
   - Fibonacci, factorielle
   - Manipulation strings
   - Data structures

3. Web (HTML/CSS/JS) (5k)
   - GÃ©nÃ©ration HTML
   - Styles CSS
   - Fonctions JavaScript simples

4. Data processing (5k)
   - Listes, dictionnaires
   - Parsing JSON
   - Calculs sur arrays


PROMPT POUR GÃ‰NÃ‰RATION (GPT-4/Claude):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GÃ©nÃ¨re 20,000 exemples de conversations oÃ¹ un assistant doit utiliser du code
pour rÃ©pondre Ã  l'utilisateur.

Format de chaque exemple:
{
  "messages": [
    {"role": "user", "content": "<question>"},
    {"role": "assistant", "content": "<|code|>\n<code python>\n<|end|>\n<explication>"}
  ]
}

CatÃ©gories:
- 25% calculs mathÃ©matiques
- 25% algorithmes Python
- 25% web (HTML/CSS/JS)
- 25% data processing

Exemples:
- User: "Calcule 2^10"
  Assistant: "<|code|>\nresult = 2**10\nprint(result)\n<|end|>\n2 puissance 10 Ã©gale 1024."

- User: "CrÃ©e un bouton rouge en HTML"
  Assistant: "<|code|>\n<button style='background-color: red;'>Cliquez ici</button>\n<|end|>\nVoici un bouton rouge."

GÃ©nÃ¨re les 20,000 exemples au format JSON.


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TIMELINE COMPLET
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Phase 1: Model 1 (0.5B Factual)
â”œâ”€ Pretrain:           5 jours  (60h GPU)
â”œâ”€ SFT #1:             12h GPU
â”œâ”€ Merge #1:           10 min CPU
â”œâ”€ SFT #2 (routing):   3h GPU
â””â”€ Merge #2:           10 min CPU
   Total:              ~5.7 jours GPU

Phase 2: Model 2 (0.3B Code)
â”œâ”€ Pretrain:           3-4 jours (36-48h GPU)
â”œâ”€ SFT:                8h GPU
â””â”€ Merge:              10 min CPU
   Total:              ~4 jours GPU

Phase 3: Router + Sandbox
â”œâ”€ Sandbox:            2-3h dev
â”œâ”€ Router:             3-4h dev
â””â”€ Testing:            2-3h
   Total:              1 jour dev

TOTAL PROJET:          ~10-11 jours GPU + 1 jour dev


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
SCRIPTS Ã€ CRÃ‰ER
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Scripts existants (dÃ©jÃ  crÃ©Ã©s):
âœ… download_ultra_filtered_mistral.py
âœ… pretrain_hessgpt_mistral.py
âœ… sft_hessgpt_mistral_lora.py
âœ… HessGpt.py (architecture)

Scripts Ã  crÃ©er:
ğŸ“ merge_lora.py                    - Fusionner LoRA dans base model
ğŸ“ download_code_dataset.py         - Download datasets code (The Stack, etc.)
ğŸ“ config_model2.json               - Config pour Model 2 (0.3B)
ğŸ“ sandbox.py                       - Python sandbox sÃ©curisÃ©
ğŸ“ router.py                        - Router Model 1 â†” Model 2
ğŸ“ inference.py                     - Inference avec routing
ğŸ“ generate_routing_dataset.py     - Helper pour gÃ©nÃ©rer 20k samples


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STRUCTURE FICHIERS FINALE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

HessGPT_Project/
â”œâ”€â”€ Core/
â”‚   â””â”€â”€ Model/
â”‚       â”œâ”€â”€ HessGpt.py
â”‚       â”œâ”€â”€ attention.py
â”‚       â”œâ”€â”€ transformer_block.py
â”‚       â””â”€â”€ feedforward.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_ultra_filtered_mistral.py
â”‚   â”œâ”€â”€ download_code_dataset.py
â”‚   â”œâ”€â”€ pretrain_hessgpt_mistral.py
â”‚   â”œâ”€â”€ sft_hessgpt_mistral_lora.py
â”‚   â”œâ”€â”€ merge_lora.py
â”‚   â”œâ”€â”€ router.py
â”‚   â”œâ”€â”€ sandbox.py
â”‚   â””â”€â”€ inference.py
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ model1_config.json          (0.5B factual)
â”‚   â””â”€â”€ model2_config.json          (0.3B code)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ultra_filtered/             (Model 1 pretrain)
â”‚   â”œâ”€â”€ code_filtered/              (Model 2 pretrain)
â”‚   â”œâ”€â”€ sft_conversational/         (Model 1 SFT #1)
â”‚   â””â”€â”€ routing_dataset_20k.json    (Model 1 SFT #2)
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ HessGpt_model1_final.pt     âœ… Model 1 final
â”‚   â”œâ”€â”€ HessGpt_model2_final.pt     âœ… Model 2 final
â”‚   â””â”€â”€ [intermediate checkpoints]
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ ARCHITECTURE_OVERVIEW.txt
â””â”€â”€ TRAINING_ROADMAP.txt            (ce fichier)


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
NOTES IMPORTANTES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš¡ CRITIQUES:
1. Dataset routing (20k samples) est ESSENTIEL
   â†’ Sans Ã§a, Model 1 ne saura pas quand utiliser <|code|>

2. Sandbox DOIT Ãªtre sÃ©curisÃ©
   â†’ Timeout, limite mÃ©moire, pas d'accÃ¨s filesystem

3. Model 2 doit Ãªtre spÃ©cialisÃ© code UNIQUEMENT
   â†’ Pas de data factuelle, sinon dilution

4. Token <|code|> suffit (pas besoin de </code>)
   â†’ <|end|> sert de dÃ©limiteur


âœ… AVANTAGES:
- Model 1 reste lÃ©ger et rapide pour conversations
- Model 2 expert en code/math
- ExÃ©cution sandbox sÃ©curisÃ©e
- CoÃ»t total: ~10-11 jours GPU (acceptable)


ğŸ¯ PRIORITÃ‰S:
1. Finir training Model 1 (Phase 1) - 5.7 jours
2. GÃ©nÃ©rer dataset routing 20k - 1-2 jours
3. CrÃ©er sandbox + router - 1 jour
4. Tester inference avec routing
5. Si tout OK â†’ Train Model 2 (Phase 2) - 4 jours


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                            CrÃ©Ã©: FÃ©vrier 2025
                        Auteur: HessGPT Project Team
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
